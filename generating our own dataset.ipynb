{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd85750",
   "metadata": {},
   "source": [
    "# Moving Forward: Generating Our Own Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8585a",
   "metadata": {},
   "source": [
    "## Mistakes were made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2778f2b1",
   "metadata": {},
   "source": [
    "The available dataset was a disastrous failure. There are multiple reasons why using a ready dataset was not good for our task:\n",
    "1. **Cleanliness:** The audio provided by datasets is way too perfect and clean. It does not match the prediction data at all, where the prediction data could be the full track, or a source-separated drum-track courtesy of another neural net. I am planning on using demucs v4, which yields the best results for separation quality currently.\n",
    "2. **Class imbalance:** The dataset had a *horrendous* imbalance coefficient of 49 (meaning that there were 49 times as many snare drums as there were crash cymbals) $$Imbalance\\ Coefficient = \\frac{N_{\\text{majority}}}{N_{\\text{minority}}} =\\frac{N_{\\text{snare}}}{N_{\\text{crash}}}=\\frac{125049}{2552}\\approx 49.01$$\n",
    "3. **MIDI mismatch:** After listening to some stitched audio recordings of crash cymbals I acquired after processing the dataset, I have noticed that some onsets that were given by the MIDI were incredibly wrong. After setting a cutoff value of `50` for velocity, it helped with weeding out \"ghost\" crash cymbals that were very quiet, but reduced the duration of **ALL** available crash cymbals to a mere 7 seconds. It led me to also notice that the velocity value does not accurately represent the volume in the waveform, meaning that there could be loud crashes with a low velocity. Thus, there is an irreperable difference between the .wav and .midi files, which I assumed to be perfectly in line with one another.\n",
    "\n",
    "All of this leads to one solution which I dreaded of even thinking about: **labeling my own dataset.** As scary as it sounds, it might not be as bad as you think. Here is the general outline I will follow to generate my own drumming dataset:\n",
    "1. Select an audio source separating algorithm.\n",
    "2. Install CUDA compatible with the latest PyTorch (currently it is CUDA 11.7)\n",
    "3. Gather several fully instrumented audio files representative of different genres, tempos and styles.\n",
    "4. Separate the drum tracks from the audio files and save them for the future dataset.\n",
    "5. Record all drum track *file paths* in a `master.csv` file to be accessed later.\n",
    "6. Accessing the .csv, use an onset detection algorithm to find instances of drum notes in each the drum tracks. I will be using the `librosa` package for this task.\n",
    "7. Save the onsets as a .csv and register their name in the `master.csv` file.\n",
    "8. Concatenate the entire list of onsets and verify if you have enough to label.\n",
    "9. Start the labeling process by using the `pigeon` package to quickly and iteratively go through every onset and give them all a label.\n",
    "\n",
    "I was able to get this idea of labeling my own data from Medium user [YoshiMan](https://yoshi-man.medium.com), who did a similar task a month ago and published their results in TowardsAI. \n",
    "\n",
    "* It took them 5 hours to manually label **4,513** drum notes, which equates to roughly 900 labels per hour.\n",
    "* The small dataset I used had **400,000** drum notes, and it was **6GB** of audio in size. \n",
    "* The large dataset I intended to use is **130GB** in size and has a colossal drum count of **13.1 million**.\n",
    "\n",
    "In the case of MIR (music information retrieval) tasks, the nature of the prediction data is vastly different from the audio provided by ready datasets. Thus, it is imperative to use training data that closely resembles the prediction data. In this case, using a drum track obtained through a source separation algorithm provides a much more accurate representation of the prediction data, ensuring that the trained model will perform well on real-world tasks. Although the quantity of data may suffer significantly, the quality of the data will be worth the sacrifice, as it ensures that the trained model will perform well on real-world tasks, providing a more robust and reliable solution for the given task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa6f4b",
   "metadata": {},
   "source": [
    "## Making The Perfect Playlist for My Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df45fbc",
   "metadata": {},
   "source": [
    "To ensure that my efforts to label my own dataset are not wasted, I need to choose various tracks spanning multiple genres, tempos and styles. There are several genres that come to my mind:\n",
    "\n",
    "1. **Punk Rock.** One of the very first issues I came across using a ready-made dataset was that it was incredibly imbalanced. One of the least occurent classes was the Crash Cymbal, making up a measly `0.64%` of the dataset. Punk Rock has a lot of aggressive style that utilizes a variety of crashes that should remedy my issue.\n",
    "2. **Power Metal.** A sort of evolution of the Punk Rock style, this can help us gather good data for the genre's infamously cheesy blastbeat-laden fills and choruses. This will be a good source for gathering kick drum data.\n",
    "3. **Progressive Metal.** A genre I selected due to its complex time-signatures, drum patterns and tempo relations. On top of that, has a rather varied accompaniment that can help bring extra data augmentation into the mix.\n",
    "4. **Rock and Country.** Very average genres in terms of drumming patterns, helps with the more traditional and predictable drumming.\n",
    "5. **Hip-hop.** Known for its complex drum rhythms, this will be crucial for training the model to recognize more intricate drumming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7cef60",
   "metadata": {},
   "source": [
    "## Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5d2fff",
   "metadata": {},
   "source": [
    "### Getting Drum Onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "from IPython.display import Audio, display, Image\n",
    "from ipywidgets import widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "output_path = \"separated\\htdemucs\"\n",
    "dataset_name = \"HeartsOnFire-v.1.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33265c56",
   "metadata": {},
   "source": [
    "This code searches for all mp3 files in the `dataset_name` folder and adds them to the dataset accordingly: splitting the drums, getting the onsets and saving csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8dc73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(dataset_name)\n",
    "mp3_files = [os.path.splitext(f)[0] for f in files if f.endswith('.mp3')]\n",
    "labels = ['changed', 'kick', 'snare', 'hihat', 'tom', 'crash', 'ride', 'click', 'uncertain', 'other']\n",
    "\n",
    "def generate_onsets(folder_path):\n",
    "    drum_path = os.path.join(folder_path, \"drums.mp3\")\n",
    "    y, sr = librosa.load(drum_path, sr=44100)\n",
    "\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "    return onset_times\n",
    "\n",
    "for filename in mp3_files:\n",
    "    folder_path = os.path.join(dataset_name, filename)\n",
    "    src_path = os.path.join(dataset_name, filename+\".mp3\")\n",
    "\n",
    "    #step 1: split\n",
    "    command = [\"demucs\", \"--mp3\", \"--mp3-bitrate\", \"320\", \"--two-stems=drums\", src_path]\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Generating splits for \\\"\"+filename+\"\\\" with GPU...\")\n",
    "        sp.run(command)\n",
    "\n",
    "    #step 2: move everything\n",
    "    dest_path = os.path.join(folder_path, \"audio.mp3\")\n",
    "    os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "    os.rename(src_path, dest_path)\n",
    "\n",
    "    src_path = os.path.join(\"separated/htdemucs\", filename, \"drums.mp3\")\n",
    "    dest_path = os.path.join(\"HeartsOnFire-v.1.0.0\", filename, \"drums.mp3\")\n",
    "    os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "    os.rename(src_path, dest_path)\n",
    "\n",
    "    src_path = os.path.join(\"separated/htdemucs\", filename, \"no_drums.mp3\")\n",
    "    dest_path = os.path.join(\"HeartsOnFire-v.1.0.0\", filename, \"no_drums.mp3\")\n",
    "    os.rename(src_path, dest_path)\n",
    "    demucs_path = os.path.join(\"separated/htdemucs\", filename)\n",
    "\n",
    "    if os.path.exists(demucs_path) and os.path.isdir(demucs_path) and not os.listdir(demucs_path):\n",
    "        os.rmdir(demucs_path)\n",
    "    print(\"Generating onsets...\")\n",
    "\n",
    "    #step 3: generate onsets\n",
    "    onset_times = generate_onsets(folder_path)\n",
    "    \n",
    "    #step 4: save onsets\n",
    "    csv_file = os.path.join(folder_path, \"onsets.csv\")\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"onset_time\"]+labels)\n",
    "        for onset_time in onset_times:\n",
    "            writer.writerow([onset_time]+[False]*len(labels))\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "#step 5: update the master csv\n",
    "audio_info = []\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_name):\n",
    "    for dir in dirs:\n",
    "        processed_folder = os.path.join(root, dir)\n",
    "        \n",
    "        if os.path.isdir(processed_folder):\n",
    "            audio_path = os.path.join(processed_folder, \"audio.mp3\")\n",
    "            drums_path = os.path.join(processed_folder, \"drums.mp3\")\n",
    "            no_drums_path = os.path.join(processed_folder, \"no_drums.mp3\")\n",
    "            onsets_path = os.path.join(processed_folder, \"onsets.csv\")\n",
    "            \n",
    "            if os.path.isfile(audio_path) and os.path.isfile(drums_path) and \\\n",
    "                os.path.isfile(no_drums_path) and os.path.isfile(onsets_path):\n",
    "                audio_info.append((dir, audio_path, drums_path, no_drums_path, onsets_path))\n",
    "\n",
    "# write the audio information to a CSV file\n",
    "with open(os.path.join(dataset_name, \"master.csv\"), \"w\", newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"name\", \"audio\", \"drums\", \"no_drums\", \"onsets\"])\n",
    "    writer.writerows(audio_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf09e7",
   "metadata": {},
   "source": [
    "We can check an audio file for onsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524570af",
   "metadata": {},
   "outputs": [],
   "source": [
    "drum_path = os.path.join(dataset_name, \"Toehider - Daddy Issues/drums.mp3\")\n",
    "no_drum_path = os.path.join(dataset_name, \"Toehider - Daddy Issues/no_drums.mp3\")\n",
    "audio_path = os.path.join(dataset_name, \"Toehider - Daddy Issues/audio.mp3\")\n",
    "\n",
    "y, sr = librosa.load(drum_path, sr=44100)\n",
    "y2, sr = librosa.load(audio_path, sr=44100)\n",
    "y_h, y_p = librosa.effects.hpss(y2)\n",
    "onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
    "onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "clicks = librosa.clicks(frames=onset_frames, sr=sr, length=len(y))\n",
    "\n",
    "# Create a time vector\n",
    "t = librosa.times_like(onset_env, sr=sr)\n",
    "\n",
    "# Plot the onset strength envelope and onsets\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(t, onset_env, label='Onset strength envelope')\n",
    "plt.vlines(onset_times, 0, onset_env.max(), color='r', linestyle='--', linewidth=2.0, label='Onsets')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Onset strength')\n",
    "plt.title('Onsets detected in audio file')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee412a6",
   "metadata": {},
   "source": [
    "We can listen where `librosa` placed onsets by adding clicks to our audio files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbd937",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2, sr2 = librosa.load(no_drum_path, sr=sr)\n",
    "display(Audio((y2 + y + clicks), rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0be006",
   "metadata": {},
   "source": [
    "### Reading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63d3e5",
   "metadata": {},
   "source": [
    "And now we can read all info with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ffe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dataset_name, \"master.csv\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed82752",
   "metadata": {},
   "source": [
    "## Labeling the Onsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9949f",
   "metadata": {},
   "source": [
    "Now that we have a dataset with all onsets, we need to go through every single one and label them. \n",
    "\n",
    "We need to install `pigeonXT` for its **multi-label classification functionality**, as well as its Audio playback feature. We will load an audiofile and get snippets of it at the onset to classify that onset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa52904",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['name'] == \"Toehider - I Have Little To No Memory of These Memories\"]\n",
    "selected_row = filtered_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d88e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pigeonXT import annotate\n",
    "import re\n",
    "\n",
    "\n",
    "onsets_df = pd.read_csv(selected_row['onsets'])\n",
    "onsets_df = onsets_df.rename(columns={'onset_time': 'example'})\n",
    "onsets_df['name'] = selected_row['name']\n",
    "\n",
    "drum_file, sr = librosa.load(selected_row['drums'], sr=44100)\n",
    "display(Audio(drum_file, rate=sr, autoplay=False))\n",
    "\n",
    "def display_fn(html):\n",
    "    value = html.value\n",
    "    match = re.search(r'\\d+\\.\\d+', value)\n",
    "\n",
    "    if match:\n",
    "        number_str = match.group(0)\n",
    "        onset_time = float(number_str)\n",
    "    else:\n",
    "        print('lol')\n",
    "        return html\n",
    "\n",
    "    start_sample = int(onset_time * sr)\n",
    "    end_sample = start_sample + sr//8\n",
    "    end_sample2 = start_sample + 2*sr\n",
    "    display(Audio(drum_file[start_sample:end_sample], rate=sr, autoplay=True), Audio(drum_file[start_sample:end_sample2], rate=sr))\n",
    "\n",
    "annotations = annotate(\n",
    "    onsets_df,\n",
    "    options=labels,\n",
    "    task_type='multilabel-classification',\n",
    "    display_fn=lambda filename: display_fn(filename)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92a0d0",
   "metadata": {},
   "source": [
    "**Save labeling progress by running cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d433e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = annotations.rename(columns={'example': 'onset_time'})\n",
    "output.to_csv(selected_row['onsets'], index=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24739a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "onsets_df = pd.concat([pd.read_csv(row) for row in df['onsets']])\n",
    "counts = onsets_df.select_dtypes(include=bool).sum(axis=0)\n",
    "counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e4b1d94",
   "metadata": {},
   "source": [
    "Now we can filter out any unlabeled classes and split them into validation/training sets for training our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea07719",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    onsets_path = row['onsets']\n",
    "    onsets_df = pd.read_csv(onsets_path)\n",
    "    onsets_df = onsets_df[onsets_df[labels].any(axis=1)]\n",
    "    # Shuffle the dataframe\n",
    "    onsets_df = onsets_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Assign 80% of onsets to training set, and 20% to validation set\n",
    "    n = len(onsets_df)\n",
    "    train_n = int(0.8 * n)\n",
    "    onsets_df['split'] = 'training'\n",
    "    onsets_df.loc[train_n:, 'split'] = 'validation'\n",
    "    \n",
    "    # Sort the dataframe by onset_time column\n",
    "    onsets_df = onsets_df.sort_values(by='onset_time')\n",
    "    \n",
    "    # Write the updated onsets dataframe back to the CSV file\n",
    "    onsets_df.to_csv(onsets_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
