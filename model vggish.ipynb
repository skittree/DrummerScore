{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onset_time</th>\n",
       "      <th>name</th>\n",
       "      <th>changed</th>\n",
       "      <th>kick</th>\n",
       "      <th>snare</th>\n",
       "      <th>hihat</th>\n",
       "      <th>tom</th>\n",
       "      <th>crash</th>\n",
       "      <th>ride</th>\n",
       "      <th>click</th>\n",
       "      <th>uncertain</th>\n",
       "      <th>other</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.806531</td>\n",
       "      <td>Pantera - Walk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.143220</td>\n",
       "      <td>Pantera - Walk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.479909</td>\n",
       "      <td>Pantera - Walk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.816599</td>\n",
       "      <td>Pantera - Walk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.979138</td>\n",
       "      <td>Pantera - Walk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>102.562540</td>\n",
       "      <td>Toehider - I Have Little To No Memory of These...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>102.887619</td>\n",
       "      <td>Toehider - I Have Little To No Memory of These...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>102.945669</td>\n",
       "      <td>Toehider - I Have Little To No Memory of These...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>103.026939</td>\n",
       "      <td>Toehider - I Have Little To No Memory of These...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>103.073379</td>\n",
       "      <td>Toehider - I Have Little To No Memory of These...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1184 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      onset_time                                               name  changed  \\\n",
       "0       4.806531                                     Pantera - Walk     True   \n",
       "1       5.143220                                     Pantera - Walk     True   \n",
       "2       5.479909                                     Pantera - Walk     True   \n",
       "3       5.816599                                     Pantera - Walk     True   \n",
       "4       5.979138                                     Pantera - Walk     True   \n",
       "...          ...                                                ...      ...   \n",
       "1179  102.562540  Toehider - I Have Little To No Memory of These...     True   \n",
       "1180  102.887619  Toehider - I Have Little To No Memory of These...     True   \n",
       "1181  102.945669  Toehider - I Have Little To No Memory of These...     True   \n",
       "1182  103.026939  Toehider - I Have Little To No Memory of These...     True   \n",
       "1183  103.073379  Toehider - I Have Little To No Memory of These...     True   \n",
       "\n",
       "       kick  snare  hihat    tom  crash   ride  click  uncertain  other  \\\n",
       "0      True  False  False  False  False  False  False      False  False   \n",
       "1      True  False  False  False  False  False  False      False  False   \n",
       "2      True  False  False  False  False  False  False      False  False   \n",
       "3      True  False  False  False  False  False  False      False  False   \n",
       "4      True  False  False  False  False  False  False      False  False   \n",
       "...     ...    ...    ...    ...    ...    ...    ...        ...    ...   \n",
       "1179  False   True  False  False  False  False  False      False  False   \n",
       "1180   True  False  False  False  False  False  False      False  False   \n",
       "1181   True  False  False  False  False  False  False      False  False   \n",
       "1182   True  False  False  False  False  False  False      False  False   \n",
       "1183  False   True  False  False  False  False  False      False  False   \n",
       "\n",
       "         split  \n",
       "0     training  \n",
       "1     training  \n",
       "2     training  \n",
       "3     training  \n",
       "4     training  \n",
       "...        ...  \n",
       "1179  training  \n",
       "1180  training  \n",
       "1181  training  \n",
       "1182  training  \n",
       "1183  training  \n",
       "\n",
       "[1184 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "dataset_name = \"HeartsOnFire-v.1.0.0\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(dataset_name, \"master.csv\"))\n",
    "train_df = pd.DataFrame()\n",
    "valid_df = pd.DataFrame()\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    onsets_path = row['onsets']\n",
    "    onsets_df = pd.read_csv(onsets_path)\n",
    "    train_onsets = onsets_df[onsets_df['split'] == 'training']\n",
    "    valid_onsets = onsets_df[onsets_df['split'] == 'validation']\n",
    "    \n",
    "    # Append to respective dataframes\n",
    "    train_df = pd.concat([train_df, train_onsets], ignore_index=True)\n",
    "    valid_df = pd.concat([valid_df, valid_onsets], ignore_index=True)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset class\n",
    "class DrumDataset(Dataset):\n",
    "    def __init__(self, df, master_csv_path, transform, window_size=8192):\n",
    "        self.df = df\n",
    "        self.master_df = pd.read_csv(master_csv_path)\n",
    "        self.window_size = window_size\n",
    "        self.transform = transform\n",
    "        self.audio_cache = {}\n",
    "\n",
    "        for i, row in self.master_df.iterrows():\n",
    "            audio_path = row['drums']\n",
    "            audio, sr = torchaudio.load(audio_path, format=\"mp3\")\n",
    "            if audio.shape[0] == 2:\n",
    "                audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "            self.audio_cache[row['name']] = (audio, sr)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load the onset time, label, and track name for the given index\n",
    "        row = self.df.iloc[idx]\n",
    "        onset_time = row['onset_time']\n",
    "        labels = row[['kick', 'snare', 'hihat', 'tom', 'crash', 'ride', 'click']].astype(int).values.flatten()\n",
    "        labels = torch.tensor(labels).float()\n",
    "        track_name = row['name']\n",
    "\n",
    "        audio = self.audio_cache[track_name][0]\n",
    "        sr = self.audio_cache[track_name][1]\n",
    "\n",
    "        onset_window = audio[:, int(onset_time*sr)-self.window_size//2:int(onset_time*sr)+self.window_size//2]\n",
    "        spec = self.transform(onset_window[0])\n",
    "        return spec, labels\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.RandomApply([\n",
    "    #     torchaudio.transforms.PitchShift(sample_rate=44100, n_steps=random.uniform(-1, 1)),\n",
    "    #     torchaudio.transforms.TimeStretch(hop_length=64, fixed_rate=random.uniform(0.8, 1.2)),\n",
    "    # ], p=0.5),\n",
    "    transforms.Lambda(lambda x: torch.stack([\n",
    "            torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=44100,\n",
    "                n_fft=2048,\n",
    "                hop_length=64,\n",
    "                n_mels=128\n",
    "            )(x),\n",
    "            torchaudio.transforms.Spectrogram(\n",
    "                hop_length=64,\n",
    "                n_fft=2048)(x)\n",
    "        ], dim=0)),\n",
    "    transforms.Lambda(lambda x: torch.stack([\n",
    "            x[1],\n",
    "            x[0],\n",
    "            torchaudio.transforms.AmplitudeToDB()(x[0])\n",
    "        ], dim=0))\n",
    "])\n",
    "\n",
    "#valid_dataset = DrumDataset(valid_df, \"HeartsOnFire-v.1.0.0/master.csv\", transform)\n",
    "train_dataset = DrumDataset(train_df, \"HeartsOnFire-v.1.0.0/master.csv\", transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [128, 129] at entry 0 and [1025, 129] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a, x \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(\u001b[39m8\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m specgram \u001b[39m=\u001b[39m a[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m      4\u001b[0m \u001b[39m# Display the spectrogram using matplotlib\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 42\u001b[0m, in \u001b[0;36mDrumDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     39\u001b[0m sr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_cache[track_name][\u001b[39m1\u001b[39m]\n\u001b[0;32m     41\u001b[0m onset_window \u001b[39m=\u001b[39m audio[:, \u001b[39mint\u001b[39m(onset_time\u001b[39m*\u001b[39msr)\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m:\u001b[39mint\u001b[39m(onset_time\u001b[39m*\u001b[39msr)\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[1;32m---> 42\u001b[0m spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(onset_window[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m spec, labels\n",
      "File \u001b[1;32mc:\\Work\\Final Project\\HeartsOnFire\\.venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Work\\Final Project\\HeartsOnFire\\.venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:486\u001b[0m, in \u001b[0;36mLambda.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m--> 486\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlambd(img)\n",
      "Cell \u001b[1;32mIn[9], line 50\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     42\u001b[0m         spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(onset_window[\u001b[39m0\u001b[39m])\n\u001b[0;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m spec, labels\n\u001b[0;32m     45\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m     46\u001b[0m     \u001b[39m# transforms.RandomApply([\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[39m#     torchaudio.transforms.PitchShift(sample_rate=44100, n_steps=random.uniform(-1, 1)),\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[39m#     torchaudio.transforms.TimeStretch(hop_length=64, fixed_rate=random.uniform(0.8, 1.2)),\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[39m# ], p=0.5),\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     transforms\u001b[39m.\u001b[39mLambda(\u001b[39mlambda\u001b[39;00m x: torch\u001b[39m.\u001b[39;49mstack([\n\u001b[0;32m     51\u001b[0m             torchaudio\u001b[39m.\u001b[39;49mtransforms\u001b[39m.\u001b[39;49mMelSpectrogram(\n\u001b[0;32m     52\u001b[0m                 sample_rate\u001b[39m=\u001b[39;49m\u001b[39m44100\u001b[39;49m,\n\u001b[0;32m     53\u001b[0m                 n_fft\u001b[39m=\u001b[39;49m\u001b[39m2048\u001b[39;49m,\n\u001b[0;32m     54\u001b[0m                 hop_length\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[0;32m     55\u001b[0m                 n_mels\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m\n\u001b[0;32m     56\u001b[0m             )(x),\n\u001b[0;32m     57\u001b[0m             torchaudio\u001b[39m.\u001b[39;49mtransforms\u001b[39m.\u001b[39;49mSpectrogram(\n\u001b[0;32m     58\u001b[0m                 hop_length\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[0;32m     59\u001b[0m                 n_fft\u001b[39m=\u001b[39;49m\u001b[39m2048\u001b[39;49m,\n\u001b[0;32m     60\u001b[0m                 pad\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     61\u001b[0m                 normalized\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     62\u001b[0m                 power\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)(x)\n\u001b[0;32m     63\u001b[0m         ], dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)),\n\u001b[0;32m     64\u001b[0m     transforms\u001b[39m.\u001b[39mLambda(\u001b[39mlambda\u001b[39;00m x: torch\u001b[39m.\u001b[39mstack([\n\u001b[0;32m     65\u001b[0m             x[\u001b[39m1\u001b[39m],\n\u001b[0;32m     66\u001b[0m             x[\u001b[39m0\u001b[39m],\n\u001b[0;32m     67\u001b[0m             torchaudio\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mAmplitudeToDB()(x[\u001b[39m0\u001b[39m])\n\u001b[0;32m     68\u001b[0m         ], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[0;32m     69\u001b[0m ])\n\u001b[0;32m     71\u001b[0m \u001b[39m#valid_dataset = DrumDataset(valid_df, \"HeartsOnFire-v.1.0.0/master.csv\", transform)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m train_dataset \u001b[39m=\u001b[39m DrumDataset(train_df, \u001b[39m\"\u001b[39m\u001b[39mHeartsOnFire-v.1.0.0/master.csv\u001b[39m\u001b[39m\"\u001b[39m, transform)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [128, 129] at entry 0 and [1025, 129] at entry 1"
     ]
    }
   ],
   "source": [
    "a, x = train_dataset.__getitem__(8)\n",
    "specgram = a[0].squeeze(0).numpy()\n",
    "\n",
    "# Display the spectrogram using matplotlib\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(specgram, cmap='Spectral_r', origin='lower')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x32768 and 2048x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     57\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 58\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     59\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     60\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Work\\Final Project\\HeartsOnFire\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m, in \u001b[0;36mDrumCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[0;32m     17\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)))\n\u001b[0;32m     19\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n\u001b[0;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Work\\Final Project\\HeartsOnFire\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Work\\Final Project\\HeartsOnFire\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x32768 and 2048x256)"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture\n",
    "import torch.nn as nn\n",
    "\n",
    "class DrumCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrumCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn6 = nn.BatchNorm1d(256)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 7)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional Layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "x = x.view(x.size(0), -1)\n",
    "\n",
    "def accuracy(outputs, labels, threshold=0.5):\n",
    "    with torch.no_grad():\n",
    "        preds = (outputs > threshold).float()\n",
    "        correct = (preds == labels).sum(dim=1).eq(labels.shape[1]).sum().item()\n",
    "        total = labels.shape[0]\n",
    "        return 100 * correct / total\n",
    "\n",
    "# Create data loaders for training and validation sets\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the neural network and optimizer\n",
    "device = torch.device('cuda')\n",
    "model = DrumCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_acc += accuracy(outputs, labels) * inputs.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(valid_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "            valid_acc += accuracy(outputs, labels) * inputs.size(0)\n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        valid_acc /= len(valid_loader.dataset)\n",
    "    \n",
    "    # Update the learning rate based on the validation loss\n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    # Print the training and validation loss and accuracy\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
